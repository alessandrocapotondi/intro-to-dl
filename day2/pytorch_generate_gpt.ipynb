{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e6e42c-a2a3-4ba9-be85-056035147486",
   "metadata": {},
   "source": [
    "# IMDB movie review text generation\n",
    "\n",
    "Once you have fine-tuned your model you can test it interactively with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa458fec-a1e9-4960-9a9f-c7f21d0a7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "path_to_model = \"/scratch/project_462000450/data/users/mvsjober/gpt-imdb-model/checkpoint-5000/\"\n",
    "generator = pipeline(\"text-generation\", model=path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ecc40-1c1d-4c9d-a41c-937bbbbaf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    for item in output:\n",
    "        text = item['generated_text']\n",
    "        text = text.replace(\"<br />\", \"\\n\")\n",
    "        print('-', text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf677501-f93d-46b1-a618-0fb792cd44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(\"This movie was\")\n",
    "print_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb8536-887f-4fad-a0b3-190d1749a594",
   "metadata": {},
   "source": [
    "## Experiment with the generation strategy\n",
    "\n",
    "You can play with the text generation if you wish. Text generation strategies are discussed here: https://huggingface.co/docs/transformers/generation_strategies\n",
    "\n",
    "The `generator()` function has some parameters than can be tweaked:\n",
    "\n",
    "> max_new_tokens: the maximum number of tokens to generate. In other words, the size of the output sequence, not including the tokens in the prompt. As an alternative to using the output’s length as a stopping criteria, you can choose to stop generation whenever the full generation exceeds some amount of time. To learn more, check StoppingCriteria.\n",
    "> \n",
    "> num_beams: by specifying a number of beams higher than 1, you are effectively switching from greedy search to beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that start with a lower probability initial tokens and would’ve been ignored by the greedy search.\n",
    "> \n",
    "> do_sample: if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.\n",
    "> \n",
    "> num_return_sequences: the number of sequence candidates to return for each input. This option is only available for the decoding strategies that support multiple sequence candidates, e.g. variations of beam search and sampling. Decoding strategies like greedy search and contrastive search return a single output sequence.\n",
    "\n",
    "Here is a nice blog post explaining in more detail about the different generation strategies: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816b3f3-9a0f-4ca8-a7d9-d7962b0207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(\"This movie was awful because\", num_return_sequences=4, max_new_tokens=100, do_sample=True)\n",
    "print_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df008ff8-cb03-488f-b643-4aa2314de52c",
   "metadata": {},
   "source": [
    "## Compare with the original model without fine-tuning\n",
    "\n",
    "We can also load the original `distilgpt2` model and see how it would have worked without fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1f550-970e-419a-aaff-d4e821bacc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_orig = pipeline(\"text-generation\", model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995c393-29ad-4df1-b01a-83cd85008297",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator_orig(\"This movie was awful because\", num_return_sequences=4, max_new_tokens=100, do_sample=True)\n",
    "print_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23aecd9-4a56-4b06-8a65-4fc43b0628d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
